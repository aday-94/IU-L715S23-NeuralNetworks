import re 
import numpy as np

test_text = '''where are you?
is she in mexico?
i am in greece.
she is in mexico.
is she in england?
'''

train_text = '''are you still here?
where are you?
he is in mexico.
are you tired?
i am tired.
are you in england?
were you in mexico?
is he in greece?
were you in england?
are you in mexico?
i am in mexico.
are you still in mexico? 
are you in greece again?
she is in england.
he is tired.
'''

#s = str(train_text.split('\n'))

def tokenise(s):
    return [i for i in re.sub('([.?])', ' \g<1>', s).strip().split(' ') if i]

#tokenized_s = tokenise(s)
#print(tokenized_s)

def one_hot(y, classes):
    onehot = np.zeros((len(y), classes)) # creates matrix of ? rows, ? columns 
    
    # Iterate through y and update onehot's column to 1 based on the class
    # y [0, 1, 4, 3, 2]
    for i, v in enumerate(y):
        onehot[i][v] = 1
    return onehot

vocab = list(set([token for token in re.sub('([.?])', ' \g<1>', train_text)
             .replace(' ', '\n').strip().split('\n') if token]))
vocab += ['<BOS>', '<EOS>', '<PAD>']
vocab.sort()

word2idx = {word: idx for idx, word in enumerate(vocab)}
idx2word = {idx: word for word, idx in word2idx.items()}

print(word2idx)
print(idx2word)

def relu(x): # You're literally just comparing it with 0 and if its a negative number 0 is bigger otherwise its smaller
    return 0 if x < 0 else x

def softmax(z):
    #take each number in the matrix and make it e^that_number
    expz = np.exp(z) # 100x5
    #sum of all values in the row
    sumexpz = np.sum(expz, axis = 1) # 100x1
    
    # we are transposing because we want the number of columns in the first 
    # matrix to be equal to the number of rows in the second matrix
    
    #this is where the normalization occurs
    #we haven't lost each individual value despite summing each row, 
    #and then we put it over the sum to normalize it
    sigma = np.transpose(expz) / sumexpz # 5x100 / 100x1 = 5x100
    return np.transpose(sigma) # tranpose to get 100x5

pad = max([len(tokenise(i)) for i in train_text.split('\n')]) + 1
train_sentences = []
for line in train_text.strip().split('\n'):
        tokens = tokenise(line)
        padded = ['<BOS>'] + tokens + ['<EOS>'] + ['<PAD>'] * (pad - len(tokens))
        train_sentences.append([word2idx[token] for token in padded])

X = []
y = []

for sentence in train_sentences:
    for i in range(pad - 2):
            X.append([sentence[i], sentence[i+1]])
            y.append(sentence[i+2])
                  
X = np.array(X)
Y = np.array(y)
Yo = one_hot(Y, len(vocab))
X = np.vstack([X, [1,1]])

print(X.shape)
print(Y.shape)
print(Yo.shape)

ihw = np.random.randn(2,7) # These are the weights from x (input) to the hidden layer
how = np.random.randn(7, len(vocab)) # These are the weights from the hidden layer to output

E = np.random.randn(2, 4)
h = np.random.randn(4, 6) 
O = np.random.randn(6, len(vocab)) 
print(E.shape)
print(h.shape)
print(O.shape)