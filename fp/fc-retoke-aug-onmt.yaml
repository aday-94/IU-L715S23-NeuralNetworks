# The general configuration of the training process.

#log_file: fc-retoke-model.log

#data:
  # Path to the source files.
#  src:
#    - toke-src-1.txt
#    - toke-src-5.txt
#    - toke-src-8.txt
  # Path to the target files.
#  tgt:
#    - toke-tgt-1.txt
#    - toke-tgt-5.txt
#    - toke-tgt-8.txt
  # Vocabulary size.
#  vocab:
#    src: toke-vocab.txt
#    tgt: toke-vocab.txt
# toy_en_de.yaml

## Where the samples will be written
save_data: /N/u/ad7/Carbonate/tokenization_experiments/run/toke.aug.data
## Where the vocab(s) will be written
src_vocab: /N/u/ad7/Carbonate/tokenization_experiments/run/toke.aug.vocab.src
tgt_vocab: /N/u/ad7/Carbonate/tokenization_experiments/run/toke.aug.vocab.tgt
# Prevent overwriting existing files in the folder or allow it
overwrite: True

# Corpus opts:
data:
    train_1:
        path_src: /N/u/ad7/Carbonate/tokenization_experiments/toke-src-1.txt
        path_tgt: /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-1.txt
    
    train_2:
        path_src: /N/u/ad7/Carbonate/tokenization_experiments/toke-src-8.txt
        path_tgt: /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-8.txt

    train_aug_1:
        path_src: /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-aug-1.txt
        path_tgt: /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-1.txt

    train_aug_2:
        path_src: /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-aug-8.txt
        path_tgt: /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-8.txt

    test:
        path_src: /N/u/ad7/Carbonate/tokenization_experiments/toke-src-5.txt
        path_tgt: /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-5.txt

#    valid:
#        path_src: /N/u/ad7/Carbonate/tokenization_experiments/toke-src-8.txt
#        path_tgt: /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-8.txt

# Path to the source files.
#  src:
#    - /N/u/ad7/Carbonate/tokenization_experiments/toke-src-1.txt
#    - /N/u/ad7/Carbonate/tokenization_experiments/toke-src-5.txt
#    - /N/u/ad7/Carbonate/tokenization_experiments/toke-src-8.txt
  # Path to the target files.
#  tgt:
#    - /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-1.txt
#    - /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-5.txt
#    - /N/u/ad7/Carbonate/tokenization_experiments/toke-tgt-8.txt
  # Vocabulary size.
#  vocab:
#    src: toke-vocab.txt
#    tgt: toke-vocab.txt

# Vocabulary files that were just created
src_vocab: /N/u/ad7/Carbonate/tokenization_experiments/run/toke.aug.vocab.src
tgt_vocab: /N/u/ad7/Carbonate/tokenization_experiments/run/toke.aug.vocab.tgt

# Train on a single GPU
world_size: 1
gpu_ranks: [0]

# Evaluation settings
#save_predictions: /N/u/ad7/Carbonate/tokenization_experiments/run/cnn_rnn_pred.txt
#batch_size: 16
metrics: accuracy
#  replace_unk: true

# Evaluation metrics


# Training Model
save_model: /N/u/ad7/Carbonate/tokenization_experiments/run/mean_rnn_aug/mean_rnn_aug_model
log_file: /N/u/ad7/Carbonate/tokenization_experiments/run/mean_rnn_aug/mean_rnn_aug_model.log
save_checkpoint_steps: 500
train_steps: 1000
valid_steps: 500
#train_epochs:
batch_size: 16
model_task: seq2seq
model_type: text
encoder_type: mean
decoder_type: rnn
rnn_type: LSTM
src_char_input: True
#src_char_vocab: /N/u/ad7/Carbonate/tokenization_experiments/run/toke.vocab.src